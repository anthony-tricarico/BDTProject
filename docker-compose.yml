services:
  db:
    build:
      context: ./postgresql
    restart: no
    shm_size: 128mb
    environment:
      POSTGRES_PASSWORD: example
      POSTGRES_DB: raw_data
    ports:
      - 5432:5432

  mlflow-db:
    image: postgres:latest
    environment:
      POSTGRES_PASSWORD: example
      POSTGRES_DB: mlflow
    ports:
      - 5433:5432
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  adminer:
    image: adminer
    restart: no
    ports:
      - 8080:8080

  mongodb:
    image: mongo
    container_name: mongodb
    ports:
      - 27017:27017

  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
    image: confluentinc/cp-kafka:7.3.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
  redis:
    image: redis:6
    ports:
      - "6379:6379"
  kafka-producer-passengers:
    build:
      context: ./kafka-components
      dockerfile: ./kafka-producer-passengers/Dockerfile
    env_file:
      - path: ./kafka-components/kafka-producer-passengers/.env
        required: false
    depends_on:
      - kafka
      - db
    environment:
      - SLEEP=0.05
      - BUSES=selected

  kafka-consumer-passengers:
    build:
      context: ./kafka-components
      dockerfile: ./kafka-consumer-passengers/Dockerfile
    depends_on:
      - kafka
      - kafka-producer-passengers
    restart: on-failure
    ports:
      - 8000:8000

  kafka-producer-tickets:
    build:
      context: ./kafka-components
      dockerfile: ./kafka-producer-tickets/Dockerfile
    depends_on:
      - kafka
      - kafka-consumer-passengers # for dependency on stream API data for passengers
    environment:
      - SLEEP=0

  kafka-consumer-tickets:
    build:
      context: ./kafka-components
      dockerfile: ./kafka-consumer-tickets/Dockerfile
    depends_on:
      - kafka
      - kafka-producer-tickets
      - kafka-consumer-passengers
    ports:
      - 8001:8001

  kafka-producer-sensors:
    build:
      context: ./kafka-components
      dockerfile: ./kafka-producer-sensors/Dockerfile
    depends_on:
      - kafka
      - kafka-consumer-passengers
    environment:
      - SLEEP=0

  kafka-consumer-sensors:
    build:
      context: ./kafka-components
      dockerfile: ./kafka-consumer-sensors/Dockerfile
    depends_on:
      - kafka
      - kafka-consumer-passengers
      - kafka-producer-sensors
    ports:
      - 8002:8002

  mongodb-save:
    build: ./mong
    depends_on:
      - kafka-consumer-sensors
      - mongodb
    ports:
      - 8003:8003

  kafka-producer-gps:
    build:
      context: ./kafka-components
      dockerfile: ./kafka-producer-gps/Dockerfile
    depends_on:
      - kafka
      - kafka-consumer-passengers
      - db
    environment:
      - SLEEP=0
    restart: no

  kafka-producer-traffic:
    build:
      context: ./kafka-components
      dockerfile: ./kafka-producer-traffic/Dockerfile
    env_file:
      - path: ./kafka-components/kafka-producer-traffic/.env
        required: false
    depends_on:
      - kafka
      - kafka-consumer-passengers
      - db
    environment:
      - SLEEP=1
    restart: no

  kafka-producer-weather:
    build:
      context: ./kafka-components
      dockerfile: ./kafka-producer-weather/Dockerfile
    depends_on:
      - kafka
      - kafka-consumer-passengers
      - db
    environment:
      - SLEEP=0
    restart: no

  kafka-consumer-weather:
    build:
      context: ./kafka-components
      dockerfile: ./kafka-consumer-weather/Dockerfile
    depends_on:
      - kafka
      - db

  spark-streaming:
    build:
      context: ./spark-components
      dockerfile: ./Dockerfile
    depends_on:
      - kafka
      - db
      - mongodb
    environment:
      - SPARK_LOCAL_IP=spark-streaming
    restart: on-failure

  sql-cron:
    build: ./postgresql/sql-scripts
    depends_on:
      - db
      - spark-streaming
    environment:
      - DB_NAME=raw_data
      - DB_USER=postgres
      - DB_HOST=db
      - DB_PORT=5432
      - DB_PASSWORD=example

  kafka-consumer-traffic:
    build:
      context: ./kafka-components
      dockerfile: ./kafka-consumer-traffic/Dockerfile
    depends_on:
      - kafka
      - kafka-producer-traffic

  kafka-consumer-gps:
    build:
      context: ./kafka-components
      dockerfile: ./kafka-consumer-gps/Dockerfile
    depends_on:
      - kafka
      - kafka-consumer-passengers
      - kafka-producer-gps

  minio:
    image: minio/minio
    command: server /data
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
    ports:
      - "9000:9000"
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  ml-model:
    build:
      context: ./ml-model
      dockerfile: ./Dockerfile
    depends_on:
      - kafka
      - db
      - minio

  kafka-consumer-model:
    build:
      context: ./kafka-components
      dockerfile: ./kafka-consumer-model/Dockerfile
    depends_on:
      minio:
        condition: service_healthy
      kafka:
        condition: service_started
      ml-model:
        condition: service_started

  prediction-service:
    build:
      context: ./prediction-service
      dockerfile: ./Dockerfile
    ports:
      - "8006:8006"
    environment:
      - MODEL_RELOAD_INTERVAL=300
    depends_on:
      minio:
        condition: service_healthy
      kafka-consumer-model:
        condition: service_started

  streamlit:
    build:
      context: ./streamlit
      dockerfile: ./Dockerfile
    ports:
      - "8501:8501"
    depends_on:
      - db
      - minio
      - prediction-service

  dask-scheduler:
    build:
      context: ./ml-model
      dockerfile: Dockerfile.dask
    command: ["dask-scheduler"]
    ports:
      - "8786:8786"
      - "8787:8787"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8787/status"]
      interval: 10s
      timeout: 5s
      retries: 3

  dask-worker:
    build:
      context: ./ml-model
      dockerfile: Dockerfile.dask
    command:
      [
        "dask-worker",
        "dask-scheduler:8786",
        "--nthreads",
        "2",
        "--memory-limit",
        "4GB",
      ]
    depends_on:
      - dask-scheduler
    environment:
      - PYTHONPATH=/home/dask
      - DASK_DISTRIBUTED__WORKER__MULTIPROCESSING_METHOD=fork
    deploy:
      replicas: 2

  training-service:
    build:
      context: ./ml-model
      dockerfile: Dockerfile.training
    depends_on:
      - db
      - kafka
      - mlflow-db
      - mlflow
      - dask-scheduler
    ports:
      - "8007:8007"
    environment:
      - POSTGRES_URL=postgresql://postgres:example@db:5432/raw_data
      - MLFLOW_TRACKING_URI=http://mlflow:5001
      - MLFLOW_DB_URL=postgresql://postgres:example@mlflow-db:5432/mlflow
    restart: unless-stopped

  mlflow:
    build:
      context: ./mlflow
      dockerfile: ./Dockerfile
    ports:
      - "5001:5001"
    environment:
      - MLFLOW_TRACKING_URI=postgresql://postgres:example@mlflow-db:5432/mlflow
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=example
      - POSTGRES_DB=mlflow
      - POSTGRES_HOST=mlflow-db
    command: >
      mlflow server 
      --host 0.0.0.0 
      --port 5001
      --backend-store-uri postgresql://postgres:example@mlflow-db:5432/mlflow
      --default-artifact-root /mlflow/artifacts
    volumes:
      - ./mlflow/artifacts:/mlflow/artifacts
    depends_on:
      mlflow-db:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  minio_data:
