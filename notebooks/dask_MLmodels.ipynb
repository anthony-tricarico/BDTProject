{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Provided index column is of type \"object\".  If divisions is not provided the index column type must be numeric or datetime.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m engine = create_engine(POSTGRES_URL)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Read tables into Dask DataFrames\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m tickets = \u001b[43mdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_sql_table\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mraw_tickets\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPOSTGRES_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mticket_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnpartitions\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m sensors = dd.read_sql_table(\u001b[33m'\u001b[39m\u001b[33mraw_sensors\u001b[39m\u001b[33m'\u001b[39m, POSTGRES_URL, index_col=\u001b[33m'\u001b[39m\u001b[33mmeasurement_id\u001b[39m\u001b[33m'\u001b[39m, npartitions=\u001b[32m10\u001b[39m)\n\u001b[32m     14\u001b[39m trips = dd.read_sql_table(\u001b[33m'\u001b[39m\u001b[33mtrips\u001b[39m\u001b[33m'\u001b[39m, POSTGRES_URL, index_col=\u001b[33m'\u001b[39m\u001b[33mtrip_id\u001b[39m\u001b[33m'\u001b[39m, npartitions=\u001b[32m10\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/UniTrento/Courses/Second Semester/BD/Technologies/BDTProject/.venv/lib/python3.13/site-packages/dask/dataframe/io/sql.py:341\u001b[39m, in \u001b[36mread_sql_table\u001b[39m\u001b[34m(table_name, con, index_col, divisions, npartitions, limits, columns, bytes_per_chunk, head_rows, schema, meta, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m    337\u001b[39m     columns.append(index)\n\u001b[32m    339\u001b[39m query = sql.select(*columns).select_from(table_name)\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_sql_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdivisions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdivisions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnpartitions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnpartitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbytes_per_chunk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbytes_per_chunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_rows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/UniTrento/Courses/Second Semester/BD/Technologies/BDTProject/.venv/lib/python3.13/site-packages/dask/dataframe/io/sql.py:171\u001b[39m, in \u001b[36mread_sql_query\u001b[39m\u001b[34m(sql, con, index_col, divisions, npartitions, limits, bytes_per_chunk, head_rows, meta, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m    169\u001b[39m         divisions = np.linspace(mini, maxi, npartitions + \u001b[32m1\u001b[39m, dtype=dtype).tolist()\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    172\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mProvided index column is of type \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m.  If divisions is not provided the \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    173\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mindex column type must be numeric or datetime.\u001b[39m\u001b[33m\"\u001b[39m.format(dtype)\n\u001b[32m    174\u001b[39m         )\n\u001b[32m    176\u001b[39m parts = []\n\u001b[32m    177\u001b[39m lowers, uppers = divisions[:-\u001b[32m1\u001b[39m], divisions[\u001b[32m1\u001b[39m:]\n",
      "\u001b[31mTypeError\u001b[39m: Provided index column is of type \"object\".  If divisions is not provided the index column type must be numeric or datetime."
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# --- Configuration ---\n",
    "POSTGRES_URL = \"postgresql+psycopg2://postgres:example@db:5432/raw_data\"\n",
    "\n",
    "# --- Load Data from PostgreSQL ---\n",
    "engine = create_engine(POSTGRES_URL)\n",
    "\n",
    "# Read tables into Dask DataFrames\n",
    "tickets = dd.read_sql_table('raw_tickets', POSTGRES_URL, index_col='ticket_id', npartitions=10)\n",
    "# sensors = dd.read_sql_table('raw_sensors', POSTGRES_URL, index_col='measurement_id', npartitions=10)\n",
    "# trips = dd.read_sql_table('trips', POSTGRES_URL, index_col='trip_id', npartitions=10)\n",
    "# traffic = dd.read_sql_table('traffic', POSTGRES_URL, index_col='trip_id', npartitions=10)\n",
    "# weather = dd.read_sql_table('weather', POSTGRES_URL, index_col='measurement_id', npartitions=10)\n",
    "# events = dd.read_sql_table('events', POSTGRES_URL, index_col='event_id', npartitions=10)\n",
    "\n",
    "# --- Aggregations (similar to MLmodels.ipynb) ---\n",
    "\n",
    "# Tickets: count passengers in per trip per day\n",
    "tickets['timestamp'] = dd.to_datetime(tickets['timestamp'])\n",
    "tickets['day'] = tickets['timestamp'].dt.date\n",
    "agg_tickets = tickets.groupby(['trip_id', 'day']).agg({\n",
    "    'fare': 'count',\n",
    "    'school': 'max',\n",
    "    'hospital': 'max',\n",
    "    'peak_hour': 'max',\n",
    "    'timestamp': 'max'\n",
    "}).rename(columns={'fare': 'passengers_in'}).reset_index()\n",
    "\n",
    "# Sensors: count passengers out per trip per day\n",
    "sensors['timestamp'] = dd.to_datetime(sensors['timestamp'])\n",
    "sensors['day'] = sensors['timestamp'].dt.date\n",
    "agg_sensors = sensors.groupby(['trip_id', 'day']).agg({\n",
    "    'status': 'count',\n",
    "    'timestamp': 'max'\n",
    "}).rename(columns={'status': 'passengers_out'}).reset_index()\n",
    "\n",
    "# Merge tickets and sensors\n",
    "merged = dd.merge(agg_tickets, agg_sensors, on=['trip_id', 'day', 'timestamp'], how='inner')\n",
    "\n",
    "# Merge with trips\n",
    "merged = dd.merge(merged, trips.reset_index(), on='trip_id', how='left')\n",
    "\n",
    "# Merge with traffic\n",
    "traffic['timestamp'] = dd.to_datetime(traffic['timestamp'])\n",
    "merged = dd.merge(merged, traffic.reset_index(), on=['trip_id', 'timestamp'], how='left')\n",
    "\n",
    "# Merge with weather (asof merge, so convert to pandas for this step)\n",
    "merged_pd = merged.compute()\n",
    "weather_pd = weather.compute()\n",
    "weather_pd['hour'] = pd.to_datetime(weather_pd['hour'])\n",
    "merged_pd['timestamp'] = pd.to_datetime(merged_pd['timestamp'])\n",
    "merged_pd = pd.merge_asof(\n",
    "    merged_pd.sort_values('timestamp'),\n",
    "    weather_pd[['hour', 'temperature', 'precipitation_probability', 'weather_code', 'latitude', 'longitude']].sort_values('hour'),\n",
    "    left_on='timestamp', right_on='hour', direction='backward'\n",
    ")\n",
    "\n",
    "# Merge with events (by date)\n",
    "events_pd = events.compute()\n",
    "events_pd['day_event'] = pd.to_datetime(events_pd['day_event']).dt.date\n",
    "merged_pd['day'] = pd.to_datetime(merged_pd['day']).dt.date\n",
    "final = pd.merge(events_pd, merged_pd, left_on='day_event', right_on='day', how='right')\n",
    "final['event_dummy'] = final['day_event'].notna().astype(int)\n",
    "\n",
    "# --- Save to PostgreSQL ---\n",
    "final_dd = dd.from_pandas(final, npartitions=10)\n",
    "final_dd.to_sql('feature_table', POSTGRES_URL, if_exists='replace', index=False)\n",
    "\n",
    "print(\"Feature table created and saved to PostgreSQL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
